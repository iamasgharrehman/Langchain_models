from langchain_openai import ChatOpenAI
from dotenv import load_dotenv

load_dotenv()

'''temperature values ranges from 0 to 2 and it helps define the creativty of the model
max_completion_token parameter is used to determine the max number of token generated by the llm
it helps control the pricing of our usage'''
model = ChatOpenAI(model='gpt-4', temperature=0.5)

result = model.invoke('What is the capital of Pakistan')

print(result.content)